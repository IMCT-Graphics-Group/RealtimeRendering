## 读书笔记: Real-time Rendering 4th Edition

### 0. 笔记前言

很难想象，我本来应该是看Pbrt并且深耕光线追踪的，但我却突发奇想决定先过一遍rtr，但仔细梳理逻辑又并不是没有根据的：Pbrt-v4出版在即，新版本引入了对Optix的支持，令我神往，所以先暂缓对Pbrt的学习；rtr更像一本算法目录，提供了大量有关光栅化、实时光追的内容，而我的研究内容是关于实时Pt的，也许可以通过rtr打开思路；最后一点大概就是对于工程的懈怠吧，我暂时觉得自己还没有做好为学习Pbrt做大量工程实践的准备，在这个还不算忙碌的研一下学期，抓住最后一点自由的时间，好好地享受阅读rtr的快乐吧~

### 1. 图形渲染管线

管线（或称为流水线），是一种有效提高生产效率的方式。通过将生产全流程拆解成$n$个不同的管线阶段，可以将生产速度提升$n$倍。但管线的生产效率受制于其最慢的管线阶段（木桶效应）。宏观上，渲染管线可以分成四个阶段：应用阶段→几何处理阶段→光栅化阶段→像素处理阶段。实际使用中，每一个阶段的内部又可以继续细分成更多的阶段。

- **应用阶段**：应用阶段由具体的应用驱动，通常表现为由CPU端执行的渲染前准备和渲染调度任务。由于应用阶段在CPU端执行，所以开发者具有完全的控制权，应用阶段的处理也会很大程度上影响后续阶段的执行，比如优化算法来减少需要绘制的三角形数量。应用阶段的一些工作也可以通过`Compute Shader`交给GPU处理。应用阶段最终会将需要绘制的几何信息传递给下一个阶段，这些几何信息通常描述了一些基础的图形元素，比如点、线、三角形。另外，碰撞检测、交互设备（鼠标、键盘、手柄、头显等）的信息处理也都在该阶段进行。

- **几何处理阶段**：几何处理阶段主要负责坐标变换、投射等与几何处理相关的任务。这个阶段会计算出哪些内容需要被绘制、应当怎样绘制，以及绘制到哪里。几何处理阶段一般在GPU上执行。几何处理阶段执行逐三角形和逐顶点的处理操作，这个阶段具体分为四个阶段：

  1. **顶点着色**：顶点着色阶段有两个主要任务：计算出顶点的位置、顶点输出数据（法线、纹理坐标等）。经典的物体着色都是在顶点上计算光照并存储颜色，然后在三角形中插值。这类可编程的顶点着色单元被称为顶点着色器。随着GPU硬件的发展，物体着色基本从顶点着色转换为像素着色，而原先的顶点着色器则被用作一个通用的处理阶段，用于计算各顶点的数据。顶点数据处理包含多次坐标系变换：顶点最初位于所属模型的`模型空间`，每个模型可以生成多个实例副本，每个实例都含有一个`模型变换（矩阵）`，描述了模型的缩放、位置和朝向，`模型变换（矩阵）`作用于模型的顶点和法线，变换之前的坐标系为`模型坐标系`，变换之后的坐标系为`世界坐标系`；为了便于相机投影和裁剪，所有处于`世界坐标系`下的物体都会进行`视图变换（矩阵）`，变换后的物体将处于`相机空间`，相机将位于坐标系原点，朝向$z$轴的负方向，$y$轴朝上，$x$轴朝右。除了`坐标变换`输出新的坐标之外，顶点着色还可以对顶点进行`着色`。有的渲染管线中，`着色`计算发生在几何处理阶段（逐顶点着色），有的则发生在像素处理阶段（逐像素着色）。顶点着色输出的着色数据（颜色、向量、纹理坐标等）将传递到光栅化阶段和像素处理阶段，用于插值和计算最终的表面颜色。顶点着色中，渲染管线底层还会先后执行`投影`和`裁剪`，将`视锥体`变换为从(-1,-1,-1)到(1,1,1)的立方体（或者0~1）。先执行`投影`变换，有`正交投影`和`透视投影`两种投影方式，`正交投影`是一种平行投影，常用于建筑领域，`透视投影`则是模拟人眼观察事物的方式。`视锥体`是将相机张开的角锥体保留近裁切面和远裁切面之内的部分得到的。经过`投影`变换之后，模型将处于`裁剪空间`，`裁剪坐标`是一种齐次坐标。根据齐次坐标缩放（除以$w$）后得到标准设备坐标系下的坐标（$ndc$），最后再将$ndc$坐标转换到窗口坐标。

  2. **细分着色（可选）**：为了使靠近的物体呈现更多的几何细节，可以通过细分着色器来生成更多的三角形面片。

  3. **几何着色（可选）**：几何着色器的出现早于细分着色器，所以有更多的GPU支持。和细分着色器类似，几何着色器也可以生成新的几何顶点。几何着色器可以生成的范围很有限，用得不多。最常见的情况是用于粒子系统：几何着色器将单个点转换为一个朝向相机的四边面，这样就可以方便地着色。

  4. **流输出（可选）**：使用流输出可以将处理好的顶点数据输出到一个数组里，用于其他处理，而不是送入渲染管线的后续阶段。流输出常用于粒子模拟。

     **关于裁剪**：只有裁剪空间之内的元素需要被送进下一个光栅化阶段。对于完全处于视锥体外的元素，将被直接丢弃；完全处于视锥体内的元素，将被直接送入下一个阶段；只有部分处于视锥体内的元素需要进行裁剪。举例，当一条直线部分地处于视锥体内时，将裁剪掉处于视锥体外的顶点，并用一个新的顶点替代。裁剪空间中的顶点经过齐次除法（透视除法）后，转入$ndc$空间。

- **光栅化阶段**：光栅化阶段负责将顶点组装成三角形，并且找到被各个三角形覆盖的屏幕像素，将这些信息送入下一个处理阶段。光栅化阶段一般在GPU上执行。光栅化分成两个子阶段：三角形装配、三角形遍历。虽然以上两个子阶段都被称为“三角形_xx”，但实际上也对点和线处理。光栅化阶段也被称为“扫描转换”，意味着将屏幕空间的二维坐标顶点转换为屏幕像素。如何判定某个像素被三角形覆盖，取决于开发者对管线的设置。比如单点采样的方法就是检测某个像素的中心是否位于三角形中。也可以使用多点采样方法，比如超级采样或者多重采样。（三角形装配和遍历阶段是固定函数的GPU执行阶段）

  1. **三角形装配**：这个阶段负责计算三角形各边的表达式以及其他各种数据。这些数据会在三角形遍历阶段中参与插值。
  2. **三角形遍历**：这个阶段会检查像素中心（或采样点）是否被三角形覆盖，然后生成三角形所覆盖的片元。每个三角形片元的属性都是由三角形的顶点插值得到的；透视矫正也发生在这个阶段。

- **像素处理阶段**：像素处理阶段会逐像素地执行一段程序，这段程序将计算出该像素的颜色。这个计算过程也许很复杂：可能会先执行深度测试，确定是否需要被绘制；也可能会执行颜色混合，将当前计算出的颜色和已有颜色混合。像素处理阶段一般在GPU上执行。像素处理阶段分为像素着色阶段和合成阶段。

  1. **像素着色**：可编程的着色器阶段，最终将一个或多个颜色送入下一阶段。
  2. **合成**：将像素着色阶段输出的颜色合成。也被称为**ROP**（raster operations pipeline）（render output unit）。该阶段虽然不可编程，但是高度可配置。这个阶段也会负责解决可见性问题（Z-Test&Z-Write）。早期的图形API中，也会在合成阶段处理Alpha-Test来丢弃完全透明的片元；现代的图形API则可以在可编程的阶段控制丢弃片元。模板缓冲（stencil buffer）是一个后台缓冲，用于记录各个元素的位置信息（一般是8bit每像素）。**模板缓**冲会被用于控制颜色缓冲和深度缓冲。举个例子：一个实心圆被写入了模板缓冲，后续的元素写入就可以依据该模板缓冲，仅写入实心圆所覆盖的区域。**帧缓冲（framebuffer）**是由全部缓冲组成的。

### 2. 图形处理单元

GPU采用了和CPU很不一样的处理方式。GPU芯片的绝大多数面积都用于放置数千个**shader cores**。GPU是一种**流处理器**，大量排好序的相似数据将依次执行。正是由于数据的相似性（比如顶点数据集或者像素数据集），GPU才可以最大程度地并行执行。另一个重要的因素是，GPU会尽可能地将调度彼此独立，这样就不需要相邻指令的调度信息，也不需要共享内存的写入地址。GPU对吞吐量（数据的最大处理速率）进行了专门优化，代价是GPU上减少了缓存和控制单元的面积，这也使得GPU上的延迟比CPU更高。使用同一段着色器程序的GPU线程会被捆成一组，NVIDIA称之为**warp**，AMD称之为**wavefront**。每个warp/wavefront会用多个 shader cores执行，通常是8~64个cores，通过SIMD处理技术。举例，有2000个线程需要执行，每个NVIDIA Warp如果由32个线程组成，则需要$2000\div32=62.5$个warps，也就是实际使用63个warps执行（其中有一个warp半空）这些任务。由于是SIMD，所以当一个warp中有线程遇到需要读取内存的情形时，所有的32个线程都是同时遇到的，warp中的线程会进行换出操作，先执行下一个warp的32个线程，等待内存读取完毕后再继续执行之前的线程。每个线程都有一个寄存器，用于临时存储执行状态，合适的时候可以恢复之前的执行任务。当然，warp执行换入换出操作的时机是不一定的，这和优化策略有关，由于换入换出的延迟很低，所以有许多优化技术都会用在warp的换入换出上。shader代码结构也会在很大程度上影响执行效率，如果一段shader程序**过度地使用寄存器**，那么分配给每个线程的寄存器就会变多，线程和warp的数量就会被迫减少，warp换出优化就没法使用。GPU中的warp占用率越高，意味着GPU的使用越充分，性能表现越好。另一个影响执行效率的重要因素是**动态分支**，主要由if语句和循环语句引发。当shader程序遇到if分支时，如果它们结果相同，走相同的分支，没有任何问题；但只要有一条线程走其他分支，那么整个warp都必须把涉及到的分支全部执行一遍，然后丢弃掉错误分支的结果。这类问题被称为**线程发散（thread divergence）**，即只有一小部分线程需要执行其他分支，却导致整个warp一起执行（或等待）的情况。

GPU实现了渲染管线（逻辑模型）中的几何处理、光栅化和像素处理阶段。其中最重要的是四个可编程的阶段：顶点着色、细分着色（可选）、几何着色（可选）和像素着色。现代GPU为着色器编程提供了统一的设计，这意味着四个可编程阶段使用相同的编程模型。从本质上来说，它们具有相同的指令集架构（ISA）。DirectX中称实现了该编程模型的处理核心为“通用着色器处理核心（common-shader core）”，具备这种核心的GPU被称为具备“统一着色器架构”。这种设计背后的思路是，着色处理器可以用于许多不同的情形，GPU可以统一一致地处理。比如，如果一组模型网格中含有大量的小三角形，而另一组大的四边形面片都是由两个三角形组成的，那么前者需要更多的顶点着色处理，后者则主要依赖于像素着色处理；如果区分使用顶点着色处理核心和像素着色处理核心，那么在需求不平衡的情况下，资源就会闲置；而使用统一的着色处理核心就可以由GPU平衡处理任务。

DirectX的HLSL可以被编译为中间语言（IL, or DXIL），实现硬件解耦，然后再由GPU的驱动器使用相应的ISA编译。

基础的数据类型为32-bit单精度浮点标量和向量，但向量只是着色器代码的一部分。现代GPU一般也会提供32-bit整型和64-bit浮点类型的支持。浮点向量比较常见的是位置信息(xyzw)、法线信息、矩阵行、颜色信息(rgba)、纹理坐标(uvwq)；整型常被用于计数器、索引、掩码。合成数据类型，比如结构体、数组和矩阵，也都是支持的。

一次draw call指令会调用图形API进行一组图形元素的绘制，相继地会由图形管线执行其中的shader。每个可编程的着色阶段都有两种类型的输入数据：统一输入数据，一次draw call中不会变化的数据；可变输入数据，由三角形顶点或光栅化产生。举例，像素着色会提供统一的光源数据，而三角形面的位置数据则是可变数据。纹理是一种特殊的统一数据，曾经是一张彩色图片，现在一般会被当做数组数据。

底层的虚拟机会对不同类型的数据提供不同类型的寄存器。统一数据可以获取的常量寄存器数量是远远大于可变数据的，这是由于不同顶点和像素的可变数据需要分开来单独存储，所以在寄存器数量上有着天然的限制。

 Shader的流程控制有两种：静态流程控制是以统一输入数据控制的，这意味着在一次draw call中的分支结果不会发生改变；动态流程控制则是以可变输入数据控制的，这意味着每个片元执行的代码可能都是不一样的。

#### 2.1 细分着色器（Tessellation Stage）

细分着色器分为三个部分：外壳着色器（hull shader / tessellation control shader）、细分器（tessellator）和域着色器（domain shader / tessellation evaluation shader）。

输入外壳着色器的是一些图元，这些图元由控制点组成，这些控制点以某种曲线形式（如贝塞尔）定义了细分面。外壳着色器有两个功能，第一个功能是告诉接下来的细分器有多少三角形需要生成，以及它们的相关配置；第二个功能是它可以对每个控制点进行处理，可以修改这些控制点的信息，或者增删控制点。外壳着色器将处理好的控制点和细分控制数据输出到域着色器。

细分器是一个固定函数阶段，仅用于细分着色器中，负责生成新的顶点传递给域着色器。外壳着色器会将细分器需要的信息传递过去，比如细分面的类型（三角面、四边面或者线束）、细分系数（tessellation factor）。细分系数包括两种类型：内边系数、外边系数。内边系数决定了图元内部的细分程度；外边系数决定了图元边缘挤出的分段数。

域着色器有着和顶点着色器类似的数据流模式，将每一个由细分器输入的顶点处理后生成一个相应的输出顶点。域着色器使用重心坐标计算出每个顶点的位置、法线、纹理坐标等各种信息。

#### 2.2 几何着色器（Geometry Shader）

几何着色器可以将图元转换为其他类型的图元，这是细分着色器无法实现的。比如将三角形网格的每天边转换为线段，则网格就可以转变为线框。

几何着色器的输入是一个物体和它的顶点。几何着色器也可以定义和处理扩展图元，实际上，一个三角形可以扩展三个额外顶点，一个线段也可以扩展额外的两个顶点。

几何着色器用于修改输入数据，或者做一些简单的拷贝，它不能生成任何图元。常被用于生成级联阴影、不同尺寸的粒子、为毛发渲染挤出边缘等。

几何着色器保证了输出的图元顺序与输入图元的顺序相同，这也导致几何着色器不适合在一次调用中复制或创建大量的几何体。

一次draw call中GPU可能产生绘制对象的阶段只有三个：光栅化、细分着色器和几何着色器。其中，几何着色器是最难预测的，很多移动端设备的几何着色器都是软件实现的，所以很少乃至不鼓励使用几何着色器。

#### 2.3 流输出（Stream Output）

流输出可以在光栅化之前将数据输出并存储起来，跳过后续的阶段。常被用于模拟水面流动或者粒子效果等。流输出的数据都是浮点类型，所以对显存有着明显的开销。另外，流输出按照图元存储数据，所以三角形就会在每个顶点存储多个数据，开销比较高；一种常见的处理方法是，将图元都转为点数据进行流输出。送入流输出的图元顺序是不会被改变的。

#### 2.4 像素着色器（Pixel Shader）

受益于MRT（multiple render targets）技术，一个rendering pass可以生成多张图像，比如同时生成颜色图片、ID图片和世界空间距离图片。MRT技术也带来了不同类型的渲染管线，比如延迟渲染，第一个pass存储物体的位置和材质信息，后续的pass对其进行着色。一般来说，像素着色器无法获取到相邻像素的信息，但是有一个间接方法，可以通过获取当前像素和相邻像素的梯度信息（差值）来重建相邻像素的值，但这就要求所有同组的像素着色都使用相同的指令。

DX11提供了一种可以向任意位置写入信息的缓冲类型，称为unordered access view(UAV)。最初仅允许像素着色器和计算着色器使用UAV，在DX11.1中更新为所有的可编程着色器都可以使用。该技术在OpenGL4.3中出现，称为shader storage buffer object(SSBO)。GPU使用专用的原子单元避免数据竞争的情况，但也因此会产生略微的阻塞。DX11.3提供了Rasterizer order views(ROVs)来保证执行顺序，这样就可以在像素着色器阶段编写颜色混合的算法而不再需要合成阶段，代价是会产生一些阻塞而牺牲性能。

#### 2.5 合成阶段（Merging Stage）

大部分传统管线中，模板缓冲和深度缓冲的操作都发生在该阶段。当片元可见时，颜色混合也会发生在这个阶段（对于不透明物体来说，没有颜色混合，而是颜色替换）。

但由于深度缓冲在传统流水线中的位置比较靠后，导致了大量无效的计算，所以许多GPU会在像素着色器之前进行合成阶段的测试。片元的一些信息（深度）被用于测试可见性，不可见的片元会被提前裁剪掉，这项功能被称为early-z。像素着色器中是可以对片元的深度信息进行调整的，也可以控制片元是否被丢弃，所以一旦有这些操作，为了防止冲突，early-z就不会执行，但也会使得管线效率略微下降。

#### 2.6 计算着色器（Compute Shader）

有一些平台是为了实现通用GPU计算的，比如CUDA和OpenCL，这类框架一般是使用C/C++这样的语言并扩展实现一些使用GPU的库。

DX11推出了compute shader用于通用的GPU计算，这个shader并不参与渲染管线的运作。但compute shader和渲染结合紧密，因为它由图形API调用，它使用和统一着色器相同的计算资源池。由于计算着色器的线程之间可以共享显存，有些操作在计算着色器中可能会更快。

### 3. 变换

从旋转矩阵中提取欧拉角：由于旋转矩阵$\textbf E(h,p,r)=\textbf R_z(r)\textbf R_x(p)\textbf R_y(h)$，将矩阵展开可以得到$e_{21}=sinp,\quad\frac{e_{01}}{e_{11}}=\frac{-sinr}{cosr}=-tanr,\quad\frac{e_{20}}{e_{22}}=\frac{-sinh}{cosh}=-tanh$。从而解出$p,r,h$。但如果$cosp=0$，那么$tanr和tanh$就不能正确求解，这时候可以假定$h=0，然后由\frac{e_{10}}{e_{00}}=\frac{sinr}{cosr}=tanr解出r值$。

变换矩阵行列式的值如果为负数，则该变换包含反射（负缩放）。

绕任意方向的旋转：先将任意方向**r**旋转到x轴方向，这需要构建旋转矩阵**M**，也就是找到和**r**相互垂直的向量。一种数值稳定的解法是，找到**r**最小的分量，将其设为0，交换剩下的两个分量的值，并将其中一个分量值取反。接着通过归一化和叉乘，就可以获得以**r**为轴的正交坐标系，从而建立旋转矩阵**M**。之后就可以将绕**r**旋转的问题转化为绕x轴旋转的问题，最后只需要应用$\textbf M^T$将其转回来即可。当然，Goldman还提出了一种旋转方法，他的变换矩阵可以参考书上P75。

四元数（P76）：$\hat{\textbf{q}}=(\textbf{q}_v,q_w)=iq_x+jq_y+kq_z+q_w=\textbf{q}_v+q_w=sin\phi\textbf{u}_q+cos\phi$

乘法：$\hat{\textbf{q}}\hat{\textbf{r}}=(\textbf{q}_v\times\textbf{r}_v+r_w\textbf{q}_v+q_w\textbf{r}_v,\quad q_wr_w-\textbf{q}_v\cdot\textbf{r}_v)$

加法：$\hat{\textbf{q}}+\hat{\textbf{r}}=(\textbf{q}_v+\textbf{r}_v,\quad q_w+r_w)$

共轭：$\hat{\textbf{q}}^*=(-\textbf{q}_v,q_w)$

模：$n(\hat{\textbf{q}})=\sqrt{\hat{\textbf{q}}\hat{\textbf{q}}^*}=\sqrt{\textbf{q}_v\cdot\textbf{q}_v+q_w^2}=\sqrt{q_x^2+q_y^2+q_z^2+q_w^2}$

单位量：$\hat{\textbf{i}}=(\textbf{0},1)$

求逆：$\hat{\textbf{q}}^{-1}=\frac{1}{n(\hat{\textbf{q}})^2}\hat{\textbf{q}}^*$

共轭法则：

- $(\hat{\textbf{q}}^*)^*=\hat{\textbf{q}}$
- $(\hat{\textbf{q}}+\hat{\textbf{r}})^*=\hat{\textbf{q}}^*+\hat{\textbf{r}}^*$
- $(\hat{\textbf{q}}\hat{\textbf{r}})^*=\hat{\textbf{r}}^*\hat{\textbf{q}}^*$

模法则：

- $n(\hat{\textbf{q}}^*)=n(\hat{\textbf{q}})$
- $n(\hat{\textbf{q}}\hat{\textbf{r}})=n(\hat{\textbf{q}})n(\hat{\textbf{r}})$

乘法规则：

- 分配律：$\hat{\textbf{p}}(s\hat{\textbf{q}}+t\hat{\textbf{r}})=s\hat{\textbf{p}}\hat{\textbf{q}}+t\hat{\textbf{p}}\hat{\textbf{r}}$
- 结合律：$\hat{\textbf{p}}(\hat{\textbf{q}}\hat{\textbf{r}})=(\hat{\textbf{p}}\hat{\textbf{q}})\hat{\textbf{r}}$
- 四元数乘法不服从交换律

对数运算：$log(\hat{\textbf{q}})=log(e^{\phi\textbf{u}_q})=\phi\textbf{u}_q$

指数运算：$\hat{\textbf{q}}^t=(sin\phi\textbf{u}_q+cos\phi)^t=e^{\phi t\textbf{u}_q}=sin(\phi t)\textbf{u}_q+cos(\phi t)$

将一个四维表示的点或者向量$\textbf{p}=(p_x,p_y,p_z,p_w)^T$视作四元数$\hat{\textbf{p}}$，则另一个单位四元数$\hat{\textbf{q}}=(sin\phi\textbf{u}_q,cos\phi)$可以对其进行四元数乘法$\hat{\textbf{q}}\hat{\textbf{p}}\hat{\textbf{q}}^{-1}$实现绕轴$\textbf{u}_q旋转2\phi$角度。

任何非零实数乘以$\hat{\textbf{q}}$得到的都是相同的变换，这意味着$\hat{\textbf{q}}和-\hat{\textbf{q}}$表示相同的变换，也意味着给旋转轴$\textbf{u}_q$以及实部$q_w$随意地增减负号，并不会影响四元数的旋转变换。

连续使用四元数旋转变换：$\hat{\textbf{r}}(\hat{\textbf{q}}\hat{\textbf{p}}\hat{\textbf{q}}^*)\hat{\textbf{r}}^*=(\hat{\textbf{r}}\hat{\textbf{q}})\hat{\textbf{p}}(\hat{\textbf{r}}\hat{\textbf{q}})^*=\hat{\textbf{c}}\hat{\textbf{p}}\hat{\textbf{c}}^*$

四元数转齐次矩阵：

$\textbf{M}^q=\begin{pmatrix}
&{1-s(q_y^2+q_z^2)} &{s(q_xq_y-q_wq_z)} &{s(q_xq_z+q_wq_y)} &{0}\\&{s(q_xq_y+q_wq_z)} &{1-s(q_x^2+q_z^2)} &{s(q_yq_z-q_wq_x)} &{0}\\&{s(q_xq_z-q_wq_y)} &{s(q_yq_z+q_wq_x)} &{1-s(q_x^2+q_y^2)} &{0}\\&{0} &{0} &{0} &{1}\end{pmatrix}$

其中$s=\frac{2}{(n(\hat{\textbf{q}})^2)}$

从矩阵中提取四元数则可以通过相互消元法得到，最后都和$q_{w}$相关，而$q_{w}$可以通过矩阵的迹得到：$tr(\textbf{M}^q)=4-2s(q_x^2+q_y^2+q_z^2)=4(1-\frac{q_x^2+q_y^2+q_z^2}{q_x^2+q_y^2+q_z^2+q_w^2})=\frac{4q_w^2}{(n(\hat{\textbf{q}}))^2}$

球面线性插值：$\hat{\textbf{s}}(\hat{\textbf{q}},\hat{\textbf{r}},t)=(\hat{\textbf{r}}\hat{\textbf{q}}^{-1})^t\hat{\textbf{q}}$

软件中的球面线性插值：$slerp(\hat{\textbf{q}},\hat{\textbf{r}},t)=\frac{sin(\phi(1-t))}{sin\phi}\hat{\textbf{q}}+\frac{sin(\phi t)}{sin\phi}\hat{\textbf{r}}$

其中，$cos\phi=q_xr_x+q_yr_y+q_zr_z+q_wr_w$

球面线性插值的本质是在一个单位球上构造了从$\hat{\textbf{q}}$（t=0）指向$\hat{\textbf{r}}$（t=1）的最短弧。

已知两个单位方向$\textbf{s}和\textbf{t}$，则实现$\textbf{s}转到\textbf{t}$的旋转四元数$\hat{\textbf{q}}$可以表示为：$\hat{\textbf{q}}=(\textbf{q}_v,q_w)=(\frac{1}{\sqrt{2(1+e)}}(\textbf{s}\cross\textbf{t}),\frac{\sqrt{2(1+e)}}{2})$

其中，$e=\textbf{s}\cdot\textbf{t}=cos(2\phi),\quad \norm{\textbf{s}\cross\textbf{t}}=sin(2\phi)$

上述表示法避免了两方向相近时的数值不稳定性，但当两方向相反时，仍然会出现分母为零的情况，遇到这种情况时，任何垂直于两方向的向量都可以作为转轴。

#### 3.2 顶点混合

顶点混合用于解决不同物体的衔接处在发生变换时的过渡问题。衔接处的模型可能会受到来自不同物体变换矩阵的影响。

第一种顶点混合的方法：找到所有会对目标顶点产生影响的骨骼，将它们的变换矩阵按照权重进行混合，得到对顶点的变换矩阵。

第二种顶点混合的方法：找到所有会对目标顶点产生影响的骨骼，对它们的变换矩阵作用于顶点的变换结果进行插值。

权重的和不一定为一，但这种情况一般是出现在使用一些特殊的算法，比如*morph targets*

顶点混合的一个缺点是，混合处可能会产生折叠和穿插。一种解决方案是使用“对偶四元混合（双四元混合）”，平移部分也使用一个四元数表示，这样插值的时候是沿着圆弧插值的。但对偶四元混合也会引起膨胀问题，所以还有一种“旋转中心蒙皮法”可以选择。

#### 3.3 变形

模型的变形方法通常是在一个中性模型的基础上，根据权重叠加其与不同模型之间的差值得到的。

#### 3.4 几何缓存回放

对于高质量模型的变形动画，需要缓存每一帧的顶点位置，但这对硬盘读写的要求很高，通常会采用一些优化方案。首先是对数据进行压缩，通过16-bit的整型来存储位置和纹理坐标，会产生压缩损失；其次可以通过对时间、空间重新编码来压缩数据，仅存储它们的差值。

#### 3.5 投影

- 正交投影：最简单的正交投影可以直接把z分量设为零，但一般来说，在正交投影中希望约束投影的范围。更常见的正交投影通常由六个分量描述（$l,r,b,t,n,f$）分别代表左、右、底、顶、近、远，六个平面。
- 投影矩阵将约束空间压缩成一个轴对齐的方盒，在OpenGL中，这个方盒的坐标是从(-1,-1,-1)到(1,1,1)；在DirectX中则是从(-1,-1,0)到(1,1,1)。这个方盒被称为*canonical view volume*（标准视体），其中的物体坐标被称为*normalized device coordinates*（归一化设备坐标）。使用标准视体可以使裁剪变得更容易和高效。
- 透视投影相对复杂很多，需要考虑深度的存储和矫正。

### 4. 着色

*Punctual Lights*为“点”光源（词源为拉丁语“punctus”），是指光源本身没有大小和形状，但是有一个特定的位置。“点光源”和“聚光灯源”都属于“点”光源。(point and spotlight are two different forms of punctual lights)

光照衰减的平方反比虽然很简单也很正确，但在实际使用中却有很多问题，比如当距离无限接近零时，会出现“除以零”的情况；所以一般在使用的时候，会加一个小的固定偏差值$\epsilon$，虚幻引擎使用$\epsilon=1 cm$；CryEngine和Frostbite Game Engine中使用的方法是使用$r=max(r,r_{min})$。相比于虚幻引擎的$\epsilon$可以任取，CryEngine采用的钳制方法有其物理意义，小于$r_{min}$的表面点意味着它穿插进了光源体的内部，这是不可能的。另一个问题是希望当光照衰减达到一定程度的时候，可以直接裁剪掉，减少性能开销；不同的引擎给出了不同的解决方案，基本都是修正出了一个光照衰减的曲线，达到某一距离时会衰减到零。

在设计着色实现方案时，需要根据**计算频率**划分计算。首先看这个计算结果在一次draw call中是否维持不变，如果不变的话可以由应用阶段计算；如果是硬件配置或者设置信息这种很少修改的结果，可以在编译阶段就存储起来，也就不必存到shader input中了，或者也可以通过一些离线的预计算pass，在应用程序的安装和加载阶段运行；另一种情况是修改的结果不需要立即应用起来，就可以把计算摊销到多个帧上；其他的划分还有逐帧的、逐模型的、逐draw call的等，根据**计算频率**分组可以有效提高执行效率、帮助GPU优化执行策略。

如果计算结果在一次draw call中也会发生改变，那么就需要依赖可编程着色器阶段来更新了，各个可编程阶段对应不同的计算频率：

- 顶点着色器：逐（细分前）顶点
- 壳着色器：逐面块
- 域着色器：逐（细分后）顶点
- 几何着色器：逐图元
- 像素着色器：逐像素

实际上，大部分的着色计算都是逐像素的，在像素着色器里执行；其他阶段主要执行一些几何相关的计算，比如变换和变形。

即使在顶点着色器中对向量归一化，在像素着色器中插值时，仍然会产生不符合归一化的插值结果，所以需要再次归一化。另外，如果输入向量没有进行归一化，插值结果很可能不正确。鉴于以上两点因素，在进行插值的前后通常都会对向量进行归一化处理。

通常会在顶点着色器中完成一些几何信息的坐标变换，减少像素着色器中的计算开销。坐标变换的方式需要整体考虑，从性能、灵活性和简便性的角度考虑。比如场景中存在大量光源时，使用世界空间坐标可以避免对光源坐标的变换；相机空间也是常用的坐标空间，在相机空间中运算可以优化有关视线方向的运算，也可以提升一些精度。

**Shader**输入有两种类型，第一种是统一输入，由应用层提供，并且在一次draw call中全程保持不变；第二种是可变输入，可以在不同着色器之间有所改变。

**材质**是一个艺术端概念，一种材质可能对应多个shader，一个shader也可能对应多种材质。参数化材质由两部分组成：材质模板和材质实例。有些渲染框架比如虚幻，可以设置更复杂的层级材质结构，即一种材质模板可以派生自其他模板而构成多个层级。

实现着色计算过程，对着色计算进行效率优化，通常有以下几种思路：

- 找到表达式中可以化简的部分
- 区分表达式中具有不同计算频率的部分，并调整其计算时期
- 在交互和跨平台支持中取得平衡，做合适的拆分与封装

**抗锯齿**问题的本质是一个采样问题。对于连续信号，计算机中通过对采样的离散信号重建来还原原本的信号。在离散采样的过程中会产生走样问题，走样是由采样频率低于实际频率引起的，计算机图形中的走样通常呈现为边缘锯齿或者高光闪烁。为了正确地重建连续信号，采样频率必须大于原始频率的二分之一，这被称为“采样定理”（奈奎斯特定理）。

但三维空间中的点采样几乎是无解的，无论采样点之间取多近，总可以存在更小的物体无法被采样到，所以对于使用点采样渲染图像的方法来说，是不可能彻底解决走样问题的。当然，有时也可以知道一些图像的频率带宽，比如对物体表面应用纹理贴图时，可以计算出屏幕像素的采样频率和纹理贴图的采样频率，如果符合奈奎斯特定理，则贴图采样过程不需要额外的处理。

信号重建需要使用滤波器，三种常用的滤波器分别是盒式滤波器（box filter）、帐篷型滤波器（tent filter）、正弦滤波器（sinc filter）；滤波器的面积需要为一，否则会造成重建信号被放大或者缩小。

box filter重建的信号并不连续，效果很差；tent filter重建的信号虽然连续，但不平滑，所以需要引入额外的低通滤波器；最理想的低通滤波器是sinc filter，该结论由傅里叶分析给出，简单来说，频域空间最好的滤波器是box filter（消除了频率大于滤波器宽度的所有信号），将box filter转换到空间域（图像域）时需要使用正弦函数；同时，滤波器在频域的叠加也会变成在空间域的卷积。

但是，sinc filter的宽度是无限的，而且有些区域是负的，所以实际中很少使用。实际中用得更多的是对sinc filter的近似滤波器，约束了单个滤波器的作用范围。越接近sinc filter的滤波器，越可能包含产生负数的部分，所以实际会使用一些不含负数的滤波器，比如高斯滤波器。

重建得到连续信号之后，并不能直接使用，因为计算机图形只能输出离散信号。但重建得到的连续信号可以调节信号大小，在此基础上可以重新采样信号进行输出。重采样中的上采样（减少采样间隔，更多的采样点）是容易进行的，只需要按照目标采样间隔采样重建的信号即可；降采样就会复杂一些，直接降低采样数会引起走样问题，所以需要使用sinc filter滤波器。

几种屏幕空间抗锯齿技术：

- **SSAA**：超级采样抗锯齿技术，使用更高的分辨率渲染图像，然后通过滤波获得目标分辨率的图片。该方法性能开销严重。
- 一种与SSAA类似的方法是，渲染出沿x,y方向偏移半个像素的同分辨率图像，通过**累积缓存**（遗弃，可使用高精度像素颜色混合）将这四张图像叠加起来。但由于需要重新绘制场景并且多次写入缓存，性能开销也非常严重。但相比于SSAA的优势是，对累积的图像数量没有限制，可以生成质量非常高的图片。
- 但物体边缘的走样问题仍然是一个严重问题，目前的主流方法是通过**分析法**解决。在渲染过程中找到物体的边缘，将边缘的影响因素考虑进抗锯齿中。
- **MSAA**：多重采样抗锯齿技术意识到对子像素进行完整运算所造成的计算开销过高，对其进行了优化（仅在光栅化阶段对子像素的覆盖进行额外计算）。如果所有子像素都被同一个物体片元所覆盖，那么该像素的颜色直接采用像素中心点的采样值；但如果覆盖的子像素比较少，则会使用合适的权重（占比）修正该像素中心位置的着色结果。该过程基本由GPU来实现（光栅化阶段由GPU负责），所以这是一个依赖于硬件的抗锯齿算法，虽然该算法效率很高，但是也可能会产生不正确的结果，对于延迟渲染的支持也很难说。总而言之，由于只进行一次着色计算，MSAA在提高计算效率的同时也节省了大量的显存空间。基于该技术，NVIDIA提供了CSAA，AMD提供了EQAA。
- NVIDIA的TXAA技术和MFAA技术都利用了**TAA**技术，使用之前渲染的多帧画面信息进行抗锯齿。但TAA技术会使静止的场景出现颜色溢出（发光）；使快速运动的物体出现鬼影。可以通过重投影的方法修正鬼影问题，即对画面生成速度缓冲用于寻找正确的采样点。由于TAA技术的效果很好，且不需要额外的采样点，成为了近几年最受欢迎的抗锯齿技术，而之前应用最多的MSAA则因为对延迟渲染的支持欠佳而逐渐遭到淘汰。
- 采样图案对抗锯齿的影响也是显著的。奈曼提出人眼对水平和竖直方向的边缘更加敏感，对接近45°的斜边也较为敏感。基于此，旋转网格超采样（RGSS）将采样图案旋转，使得水平和竖直方向可以被更多采样点覆盖。RGSS图案实际上是一类“拉丁超立方体采样”（分层采样）。但如果图案中的采样点沿着某个方向分布，则对与该方向平行的边缘采样的效果会很差，我们通常希望得到分布更加均匀的采样图案，因此分层采样方法常常和其他一些方法结合使用，比如抖动采样、霍尔顿序列、泊松圆盘等。
- 尽管子像素的图案采样可以得到不错的效果，但仍然不理想。场景中可能会存在非常小的物体，这意味着可能没有合适的采样图案可以用。如果这些小物体本身会形成一种图案，再通过固定间隔去采样，就很可能会产生摩尔纹。一种解决方案是使用随机采样，比较典型的就是对不同的像素使用不同的采样图案。
- **形态学方法**：锯齿通常出现在边缘，比如物体边缘、阴影边缘、高光边缘。形态学是指有关结构和形状的研究，形态学抗锯齿方法主要工作在于寻找和重建边缘。基于图像的方法会遇到一些问题，首先是边缘不一定能找到，当两个物体之间的颜色差别小于算法阈值时，就很难确定物体边缘；具有高对比度、高频率元素的物体表面，也容易误导算法，从而难以确定边缘；应用形态学抗锯齿常常会影响文本质量；图像抗锯齿算法也很难处理边角位置，一种处理方法是结合MSAA的子像素覆盖来确定边缘；图像抗锯齿算法处理不同的图像的时间复杂度也是不同的，草坪可能需要天空三倍的时间。总之，图像抗锯齿算法非常节省显存和算力，所以其应用面非常广；又由于其不依赖于其他信息，非常容易从渲染管线中解耦出来。最流行的图像抗锯齿算法是FXAA和SMAA，SMAA还可以结合MSAA的采样进行更好的抗锯齿处理，两种方法都可以和TAA结合。每帧的时间开销大约是1~2毫秒。

光线穿透半透明物体的方式有很多种，在渲染领域可以简单分为两类：基于光线的、基于相机的；基于光线的半透明效果侧重于光线穿透时的衰减和散射，会使周围的其他物体也被照亮；基于相机的半透明效果侧重于渲染半透明的物体本身。

一种透明渲染方法被称为“纱门透明”，使用一种带有透明部分的棋盘格图案渲染三角形面片，使得一部分被遮挡的物体露出来。这种方法的一个主要缺陷是，只有一层透明可以被渲染得很好；这种方法的主要优势是，足够简单，它可以在任何时间按任何顺序渲染透明物体，对硬件没有额外要求。

大部分的透明算法使用的是透明度混合技术。其中的$alpha$值指的是“不透明度”，即占比为$\alpha$的光线会被物体表面阻挡。

向上混合：$\textbf{c}_o=\alpha_s\textbf{c}_s+(1-\alpha_s)\textbf{c}_d$

向上混合在处理一些透明塑料和透明玻璃时的效果比较差，因为实际中的玻璃和滤镜等透明物体也会减少光线的穿透，从而使得最终的颜色更暗一些（不透明度提升），而不是简单的颜色混合。（s=source, d=destination）

加法混合：$\textbf{c}_o=\alpha_s\textbf{c}_s+\textbf{c}_d$

加法混合对于发光效果很好用，比如闪电、火花，这些效果会使透明混合的部分更亮。加法混合对于大多数透明物体来说都不正确，但是用在多层透明的物体上（如烟雾、火焰）可以提亮颜色。

由于z-buffer只能存储一个深度信息，所以靠z-buffer是不能解决透明混合的深度问题的，仍然需要按深度顺序的排序。一种方法是对每个物体进行排序，按质心到相机的距离排序；但这种方法有很多问题，比如互相交错的物体就很难正确处理，自遮蔽也无法处理。虽然有着这样那样的问题，但这种方法胜在简单高效，仍有用武之地。实际使用中通常会关掉深度写入，这样深度测试就始终基于最近的不透明物体进行。

向下混合：$\textbf{c}_o=\alpha_d\textbf{c}_d+(1-\alpha_d)\alpha_s\textbf{c}_s\quad\textbf{a}_o=\alpha_s(1-\alpha_d)+\alpha_d$

向下混合和向上混合很相似，将目标和源对调了一下，另外修正了α值。通过向下混合可以将所有透明物体的渲染结果混合到一张颜色缓冲中，然后再使用向上混合将不透明物体的渲染结果和透明物体的渲染结果混合。

顺序无关的透明渲染（OIT）：**深度剥离**算法可以实现顺序无关的透明渲染。首先使用一个pass获取所有物体的深度信息（包括透明物体），写入第一个深度缓冲；接着第二个pass渲染所有的透明物体，找到所有命中第一个深度缓冲的透明面片，将这些面片的RGBA信息写进一个单独的颜色缓冲中，然后将深度大于第一个深度缓冲的最小深度写入第二个深度缓冲，这样就得到了第二深的面片深度信息；重复上述步骤若干次，并将颜色缓冲按向下混合做透明混合，就可以得到顺序无关的正确透明混合结果，没有交错面和自遮蔽问题。可以使用固定的次数，也可以使用pixel draw counter来确定，当该pass没有处理任何pixel时，就意味着深度剥离触底了，可以停止了。

OIT中使用向下混合的一个好处是，它最大化了近处透明物体的透明混合效果。当经过若干次混合之后，不透明度会越来越大，接近于不透明的效果，之后的物体几乎被遮挡。而向下混合会从前向后进行透明混合，可以保证上层的透明物体更加清晰。另外可以搭配使用一些剪枝优化算法，比如向下混合次数达到一定次数后就可以中止计算，几乎不会影响混合效果；反之，若使用向上混合就不太行，因为向上混合最后才计算离相机最近的像素。

OIT的计算效率比较低，需要多次pass。它有一些优化算法，比如dual depth peeling，每次剥离最近和最远的两个面；桶排序法，每个pass可以对多达32层透明面片进行排序，缺陷是需要较大的显存；另外，搭配使用MSAA也可以减少开销。

透明混合的问题不在于缺少算法，而是在于如何将算法映射到GPU上。Carpenter提出使用A-buffer存储像素所覆盖的透明片元的链表。在GPU上使用链表存储片元，使用DX11的UAVs可以实现。链表存储有时是一个优势，比如不会浪费额外的空间；有时也是一个劣势，由于没有预分配，对于一些特殊的场景，透明覆盖的链表可能很长，有些复杂的游戏场景中可能会多达50个透明面片、200个半透明粒子。

另一种OIT算法是对α算加权平均和，公式是：

$\textbf{c}_o=\sum\limits_{i=1}^{n}(\alpha_i\textbf{c}_i)+\textbf{c}_d(1-\sum\limits_{i=1}^{n}\alpha_i)$

实际运算使用拆分计算的方式（书P157），多个面根据它们的不透明度贡献颜色，不透明度越高则权重越大。

**显示编码**：通常计算各种光照和图像操作时，默认都是按照线性方式计算的；但是为了校正视觉效果，显示器的缓冲和纹理贴图一般都是使用非线性的编码方式。最简单的校正方式就是将着色器输出的颜色（范围在0~1）乘以$\frac{1}{2.2}$次方，这种方式被称为“gamma correction”（伽马校正），对于输入使用的纹理贴图和颜色，则需要乘以$2.2$次方。

最早的电子显像设备是CRT（阴极管射线显示器），这种显像设备的发光强度和电压之间呈指数关系，当作用于某个屏幕像素的能量提升时，该像素发光强度并不是线性提升的；举个例子，假设电压和发光强度之间的指数比是$2$，则将像素的电压设置为$50\%$时，实际的发光强度为$0.5^2=0.25$，而当电压设置为$100\%$时，发光强度则为$1$。虽然后来的显像设备（如液晶显示器，LCD）有其自身特定的响应曲线，但是厂商一般都会将响应曲线调校成接近CRT的响应曲线。

巧的是，这种指数关系恰好与人眼知觉的响应互为倒数，这也就使得显示编码的操作符合人的感知。这意味着，显示范围内两个邻近值的差别在编码之后是固定的。类似于人眼对比度阈值（倒数为对比敏感度）的测量方式，可以在响应曲线上各处测试$1\%$的亮度差别。这种近似最优分布减小了由于显示缓冲区精度限制而产生的条带状伪影，对于纹理贴图的使用也起到相同的优化效果。

显示转换函数可以反映显示缓冲中的数值和显示器的发光强度之间的关系，因此也被称为电光转换函数（EOTF）。这是一种和硬件相关的函数，电脑显示器、电视机、电影放映机都有关于该函数的不同标准。相反的转换函数也有一个标准函数，被用于描述摄影摄像设备，称为光电转换函数（OETF）。

当为显示器编码线性颜色值时，需要抵消显示转换函数造成的影响，这样才能产生相应的发光强度。比如说，我们的计算结果翻倍了，我们希望发光强度也翻倍，就需要应用一个显示转换函数的指数倒数来抵消非线性关系，这种消除显示设备响应曲线的处理就被称为伽马校正。当解码纹理贴图的数值时，则需要应用一个显示转换函数来获得线性值，以供后续着色处理。

总结一下着色的过程：

- 一张已经编码过的纹理贴图被传递进shader（GPU）中，由于是编码过的，也就是在线性的基础上乘以了类似于$\frac{1}{2.2}$次方，函数图像是一段斜率减小的曲线
- 为了重新得到线性值用于着色计算，需要给这张纹理贴图应用一个类似于$2.2$次方的解码函数，函数图像是一段斜率增大的曲线
- 着色计算完成之后，需要将计算出的颜色编码后写进帧缓冲中，也就是应用编码函数后传递进帧缓冲
- 最后，显示设备会将帧缓冲中的颜色刷新到屏幕上，这个过程中会天然地应用了显示转换函数（电光转换函数），这个函数相当于一个解码函数，所以最后呈现的发光强度和计算出的颜色呈线性关系
- 注意，虽然最终的颜色是线性的，但是对于人眼来说则不是。

电脑显示器的标准显示转换函数是被称为**sRGB**的色彩空间。大部分的图形API在调度GPU时，都会自动完成基于sRGB的转换，例如mipmap的生成也会按照sRGB编码。在线性空间下，双线性插值就可以正确无误地进行；alpha混合前也会先将图像解码到线性空间，混合之后再编码回去。

显示编码可以看作一种压缩方法，因为编码之后拓宽了暗部的范围，从而使得暗部数据的精度得到提升。

正如计算之后的图像需要先进行显示编码再传递给显示设备一样，摄像机拍摄的画面也需要先转换到线性空间再进行编辑。屏幕上显示的图像可以直接通过屏幕截图或者取色器获得，这类颜色会以某种图像格式存储，比如PNG、JPEG或者GIF，这种格式可以直接发送给显示器用以输出显示，不需要经过额外的转换。换句话说，所有屏幕上最终显示的图像，实际上都是经过了显示编码的，所以在着色器中使用这些图像时，需要先将它们解码回线性空间。

encode to sRGB: $y=\left\{\begin{matrix}1.055x^{1/2.4}-0.055,&{where\;x>0.0031308,}\\12.92x,&{where\;x\leq 0.0031308,} \end{matrix}\right.$

decode to linear: $x=\left\{\begin{matrix}\left(\frac{y+0.055}{1.055}\right)^{2.4},&{where\;y>0.04045,}\\\frac{y}{12.92},&{where\;y\leq0.04045,}\end{matrix}\right.$

### 5.纹理

纹理上的像素一般被称为“纹素”（texel），区别于屏幕像素。

应用纹理的流程一般是这样的：

- 确定物体模型空间内一个三维坐标点
- 通过投射器（Projector）将该三维坐标点映射到二维纹理空间，得到UV坐标
- 根据UV坐标和纹理规格，采样纹理
- 根据纹理的格式，将采样值转换到线性空间

模型师在建模的时候通常会定义好模型的展开UV，而投射器函数可以帮助模型师快速生成一个初始化的UV映射结果；投射器函数有很多种不同的投射方式，最常见的有球形投射、圆柱投射、平面投射。纹理投射的主要问题出现在模型接缝处，有一种多立方体投射方法，它将整个模型映射成多个立方体投射集合，模型的不同部分使用不同的立体方体进行投射。其他有一些投射器函数并不进行投射，它们本身其实是表面构建和细分的一个隐式部分；比如参数化的曲面模型具有天然的UV。UV坐标可以基于不同的需求建立，例如视角方向、表面温度等；使用投射器函数只是为了构建纹理坐标，通过位置信息构建只是其中一种最常见的构建方式而已。

一般来说，对一个模型应用一种投射器就足够了，但艺术家有时需要将模型拆分成多个部分，并对不同的部分应用不同的投射器。

对于实时渲染来说，投射函数通常在建模阶段使用，然后把投射的结果存储到模型文件的顶点信息中（UV坐标）。当然也有例外，有时更适合在顶点或者像素着色器中使用投射函数，可以增加投射的精度或者达到某些特殊效果，比如动画。另外有一些渲染方法，比如环境贴图映射，会使用专门的投射函数进行逐像素的计算。

纹理坐标不一定总是二维平面，有时也可能是三维体，这种情况下的纹理坐标被表示成三维向量（u,v,w），其中的w是投射方向的深度。有些系统中会将其表示成四维（s,t,r,q），其中的q作为齐次坐标的第四个值。三维体的投射类似于电影放映机，随着距离的增加，投影的纹理面积也在增大。

另一种重要的纹理坐标空间是方向空间，空间中的每一点可以通过一个方向获取。这种坐标空间的一种可视化方法是将它放在单位圆上，则球面上每一点的法线方向就是获取到这点的方向。这类纹理常常用在立方体贴图（cube map）上。

一维的纹理贴图同样值得注意，比如地形模型的颜色可能由地形高度决定，低洼的地方是绿色的，山峰则是白色的。线条也可以带有纹理，比如渲染雨就可以使用一些带有半透明图片的线条。这类纹理也常被用于值的转换，例如作为查找表。

接下来涉及到corresponder function，不知道怎么翻译好，就姑且翻译成“关联函数”吧。

在对纹理坐标进行插值之前，需要先将纹理坐标转换到纹理空间位置，这个转换函数就是“关联函数”。一种关联函数是使用图形API选择纹理中的某块区域显示，那么就只有这部分区域会被用在后续的操作中。另一种关联函数就是一个变换矩阵，可以被用于顶点或像素着色器中。

关联函数的另一个重要的控制项是图片的使用方式。图片会被按照UV映射到模型的表面，那么UV之外会是什么样？关联函数可以控制这种情况，OpenGL中的“wrapping mode”和DirectX中的“texture addressing mode”提供了一下的控制选项：

- **wrap**(DirectX), **repeat**(OpenGL)：默认选项，超出部分重复使用贴图，即UV坐标丢弃掉整数部分。
- **mirror**：每次重复使用贴图时都会进行镜像翻转。
- **clamp**：超出范围的部分使用图像边缘的值。该选项对于图像边缘采用双线性插值时很有用。
- **border**：超出范围的部分使用指定的颜色值。

以上的控制选项可以根据不同的轴单独配置，比如对U轴使用repeat，对V轴使用clamp。DirectX中还支持一种叫做**mirror once**的选项，即只对超出部分使用一次镜像翻转，之后的部分就会使用clamp，该选项对于制作对称贴花效果很有用。

重复纹理贴图是一种增加细节效果的实惠方案，但达到三次重复就容易被眼睛发现重复性；一种常见的解决周期性问题的方法是，将不同的图片结合到一起；另一种解决方案是使用shader来实现特殊的关联函数，将纹理图案随机地组合。

关联函数的另一个作用是可以轻易地在不同分辨率的图片之间切换；无论原始分辨率如何，关联函数都会完成相应的UV映射。

像素着色器中采样纹理，是通过向类似于texture2D的类型提供纹理坐标得到的，纹理坐标的范围在[0.0, 1.0]。GPU会谨慎地将纹理坐标转换为纹素的坐标位置，不同GPU的处理不完全一样，主要区别是：DirectX中左上角的纹理坐标是(0,0)，右下角的纹理坐标是(1,1)；而OpenGL中左下角的纹理坐标是(0,0)，右上角的纹理坐标是(1,1)。纹素的坐标都是整数，但有时候会想要两个纹素混合得到它们中间位置的颜色，这就引发了一个问题：像素中间的浮点值应该如何确定？有两种方案：截尾和四舍五入。DirectX 9使用四舍五入，认为每个像素的中心是(0.0, 0.0)，但这容易使人困惑，左上角像素的左下角的坐标值变成了(-0.5, -0.5)；DirectX 10中改用了OpenGL的形式，即纹素的中心坐标值为(0.5, 0.5)，然后使用截尾的方式定义纹素坐标值，例如像素(5, 9)即指u轴坐标在5.0~6.0，v轴坐标在9.0到10.0的区域。

Dependent texture read，依赖纹理读取，有两种含义。第一种含义用于移动设备上，当像素着色器使用计算出的纹理坐标而不是使用顶点着色器提供的纹理坐标获取纹理时，就会触发依赖纹理读取，任何改变纹理坐标的操作都会触发，即使是简单地交换了u轴和v轴的值；对于早期的桌面GPU，当纹理的坐标依赖于之前某张纹理的值时，就会触发依赖纹理读取，比如当纹理改变法线，进而改变对立方体贴图的采样时。

DirectX 12最大允许16384\*16384的纹理。

应用纹理主要考虑两种情形：放大和缩小。效果主要取决于使用的采样和滤波方法。

对纹理放大采样和缩小采样的问题，主要是处理对采样值的滤波方法；而抗锯齿问题主要是处理输出值的滤波方法。两者虽然都涉及滤波，但是作用于不同的阶段，不过由于大多数情况下输入和输出是线性相关的，所以对输出值滤波与采样时滤波是等价的。

#### 5.2 放大

最常见的放大技术是“最近邻法”和“双线性插值”。最近邻法有很明显的“像素化”问题，因为采样了周边最近点的像素颜色；虽然效果不好，但是最近邻法的每个像素只需要采样一个纹素即可。双线性插值法则是根据邻近的四个纹素的值，在两个方向上插值出中间值作为像素颜色，插值结果看上去比较模糊，但大幅减少了最近邻法的锯齿。

双线性插值的流程：

- 根据OpenGL表示法（像素中心位置为0.5, 0.5）修正后找到相邻的四个像素，即减去(0.5, 0.5)之后，截尾得到左下相邻像素$(x,y)$，从而得到全部的四个像素$(x,y),(x,y+1),(x+1,y),(x+1,y+1)$
- 截取的小数部分即为插值的权重$(u^{'},v^{'})$
- 用$1-u^{'}和u^{'}$插值$(x,y),(x+1,y)$
- 用$1-u^{'}和u^{'}$插值$(x,y+1),(x+1,y+1)$
- 用$1-v^{'}和v^{'}$插值以上两值

放大的纹理通常会显得比较模糊，可以在其上叠加细节纹理。细节纹理上的高频重复图案结合低频的放大纹理，可以创造出接近于高分辨率纹理的视觉效果。

双三次插值（bicubic filter）可以消除绝大多数的图像不平滑，但是计算开销比双线性插值大很多；当然，很多高阶的插值方法可以通过重复多次线性插值得到。

有一种开销比双三次插值小的算法是使用一段平滑的插值曲线进行插值。常用的两种曲线是$smoothstep$和$quintic$：

$smoothstep:s(x)=x^2(3-2x)$

$quintic:q(x)=x^3(6x^2-15x+10)$

这两种曲线都足够平滑，其中$smoothstep$满足$s^{'}(0)=s^{'}(1)=0$；$quintic$满足$q^{''}(0)=q^{''}(1)=0$。

#### 5.3 缩小

对于需要缩小纹理的情况，一个像素可能会对应多个纹素。可以像放大一样采用最近邻采样，即选用像素中心所对应的最近纹素，但该方法会产生严重的走样问题。

为了解决走样问题，就需要考虑奈奎斯特极限，即采样频率不能低于原始频率的一半。当原始频率（纹理）比较高时，要么提高每个像素的采样频率，要么降低原始频率。

解决该问题的算法很多，但基本思路是相同的：预计算纹理。

首先是**多级纹理（*mipmap*）**，这是用得最多的方案，几乎所有的图形加速结构都支持生成多级纹理。第0级纹理是原始纹理；第1级纹理是原始纹理的四分之一大小，通过对相邻的四个原始纹素取平均得到；之后每一级纹理都是上一级纹理的四分之一，直到某个边上只剩下一个纹素；这些多级纹理组成的图集一般被称为”多级纹理链“（*mipmap chain*）

多级纹理生成中需要考虑两个重要的因素：滤波和伽马校正。

一般的多级纹理滤波采用的是$2\times2$的盒式滤波，是一种效果比较差的滤波，可能会将不必处理的低频信息模糊掉，保留容易引发走样的高频信息；更好的选择是使用高斯滤波、兰佐斯滤波、Kaiser滤波，或者其他相似的滤波器。

由于纹理大多都编码进了非线性空间，如果不进行伽马校正可能会使滤波结果不正确（看上去偏暗）。但有一些纹理本身就是非线性的，对于这类纹理的滤波需要单独考虑。

确定*mipmap*的采样层级通常有两种方法，一种是使用屏幕像素覆盖的纹理四边形中的长边；另一种是求出$\part u/\part x,\part v/\part x,\part u/\part y,\part v/\part y$中的最大值。后一种方法在shader model 3.0之后可以在像素着色器中容易得到，但如果是在动态分支语句，或者顶点着色器中使用导数信息的话，就需要提前计算好或者手动计算。

访问*mipmap*时使用的是(u, v, d)三个值，其中d值不是整数，实际使用中会先根据u,v做双线性插值得到两个不同层级上的采样值，再根据d值对两个采样值做线性插值，整个采样过程被称为“三线性插值”。

*mipmap*也有一些问题，一个主要问题就是过度模糊，当视线沿着掠射角观察物体表面时，屏幕像素在两个方向上覆盖的纹理数量会有比较大的差距，有时甚至会出现一个方向上应该放大纹理，而另一方向上应该缩小纹理，由于我们在选择*mipmap*层级时，总是会按照覆盖最大的一个选，那么在这种情况下往往会得到过于模糊的效果。

然后是**Summed-Area Table**。首先需要创建和纹理一样大小的数组，但是存储颜色的精度更高（比如给每个颜色通道都分配16bits或者更多）。数组的每一个元素都存储了从纹素起点(0, 0)到当前元素映射位置所张开的矩形覆盖的纹素值之和。

SAT是一种各向异性的滤波算法，可以对非正方形的区域计算纹素值，但存储开销略大。无约束各向异性滤波算法会沿着纹理上区域的长边生成一条各向异性的平行线，在这条线上增加采样，使最终结果沿着该方向看来更加清晰。

cube map通过一个三维向量采样，表示的是从立方体中心向外发射射线所采样到的点。首先找到最大的一个分量，以此确定立方体的面（例如(-3.2, 5.1, -8.4)就采用-z面），接下来需要将剩下的分量按比例映射到[0, 1]（即(-3.2/8.4 +1)/2和(5.1/8.4 +1)/2；得到(0.31, 0.80)）

#### 5.4 纹理压缩

一张$512^2$分辨率、每个纹素使用3字节的纹理，需要占用768kB空间；如果按6:1压缩率压缩，一张$1024^2$分辨率的纹理也只需要512kB。

图片压缩的方法有很多，比如JPEG和PNG，但是这类方法不适合在GPU中实时解压缩。S3开发过一种被称为*S3 Texture Compression*的压缩算法用于GPU计算，DirectX中被称为*DXTC*，DirectX 10中被称为*BC（Block Compression）*，OpenGL中也有相应的实现，几乎所有的GPU都支持该纹理压缩算法，主要优势在于压缩后的各部分模块之间以及查找表之间都没有耦合，可以并行快速解码。

以*BC1*为例，每$4\times4$的纹素作为一个block进行压缩运算，其中的每个纹素仅存储2个2bit的插值系数，插值对象则是由指针指向的两个参考颜色，相比于无压缩的24bit颜色*BC1*压缩率可以达到6:1(即每纹素的占用空间由24bit下降至4bit)

其他还有很多的压缩方法，比如移动端常用的ETC，通过直方图归一化优化，或者换到其他的颜色空间（比如YCoCg）压缩。

#### 5.5 程序纹理

程序纹理很适合用来合成体积纹理。一种最常用的方法是，使用一个或多个噪声函数来生成纹理数值。另一种程序纹理的生成则是来自于物理模拟的结果，比如水波、裂纹扩散等。

普通纹理可以通过手动方式解决拉伸、接缝等瑕疵问题，但程序纹理就没有那么容易；程序纹理的抗走样问题既有方便的地方，也有麻烦的地方：预计算生成mipmap是没法用了；但是程序纹理有其描述函数，可以专门设计抗走样算法，比如噪声函数的频率已知，则可以规避掉可能产生走样问题的频率，还可以顺带优化性能。

#### 5.6 透明

纹理带透明通道可以提供很多有意思的用法，比如贴花（decal）、cross tree等。当使用类似cross tree的技术模拟草地的时候，需要解决大量的透明面片相互交叠的问题，一种优化方案是使用透明度测试（alpha testing），可以丢弃所有不透明度小于阈值的面片；但使用该技术时也需要注意一些问题，比如使用了mipmap后可能会进一步丢弃更多的面片，从而产生不正确的效果，这种问题可以通过修正各级mipmap的不透明度解决，具体公式参考书P204。

使用带透明度的颜色进行线性运算时，需要先进行透明度预乘，否则结果可能会不正确：

- 未进行透明度预乘：$(255,0,0,255)\otimes(0,255,0,2)=(127,127,0,128)$
- 进行过透明度预乘：$(255,0,0,255)\otimes(0,2,0,2)=(127,1,0,128)$

未进行透明度预乘可能会使贴花或透明裁切的图像产生黑边；但一些图像格式（如PNG）如果进行预乘，则会损失精度，一种解决方案是对透明区域的颜色部分填充相邻的不透明颜色值。

#### 5.7 凹凸贴图

物体表面的细节效果可以分成三类：宏观特征（覆盖许多像素）、中等特征（覆盖些许像素）和微观特征（小于一个像素）。这个分类并不严谨，在动画或某些情况下可能会从多个距离观察物体。

宏观特征一般通过建模来表现（人的肢体、头，是一种宏观特征）；微观特征一般通过着色模型来表现；中等特征介于两者之间（如皱纹、衣服接缝等），通常使用各种凹凸贴图来表现。

凹凸贴图有很多种类型，不同类型之间的区别主要在于其对细节特征的表现方式。

TBN标架（tangent, bitangent, normal）矩阵将光线从世界空间转换到切线空间。有一种节省空间的存储方法，是仅存储切线和副切线向量，但这种方法只能用在偏手性一致的矩阵中，比如很多物体都是对称的，所以制作一半的模型并将它镜像翻转，这种方案就会使两边的模型偏手性不一致；也可以额外存一个bit的数据来描述偏手性。如果TBN标架是一个正交基底，那么就可以用四元数来表示，既节省空间也能节省一些逐像素的计算，是常用的存储方式。

切线空间对于材质描述非常重要，虽然大部分的着色模型都只依赖于表面法线的方向，但是像拉丝金属、绒布之类的材质则额外需要视线、光源相对于表面的方向。

除了正常地存储切线空间法线之外，还有一些方法可以在运行时计算法线，比如使用表面位置导数和高度图导数来计算法线扰动；当然，这些方法都可能会产生瑕疵。

可以存储和使用世界空间或物体空间的法线，但这样会降低法线贴图的泛用性。切线空间法线的另一个好处是便于压缩，因为切线空间法线默认是向上的。

法线贴图的滤波比较困难，因为它通常不是线性的，如果法线滤波不正确就会产生高光闪烁。

从高度图导出法线图：首先沿x和y方向计算差分导数，即可得到未归一化的法线

$h_x(x,y)=\frac{h(x+1,y)-h(x-1,y)}{2},\quad h_y(x,y)=\frac{h(x,y+1)-h(x,y-1)}{2}$

$\textbf{n}(x,y)=(-h_x(x,y),-h_y(x,y),1)$

视差映射：根据视角方向修正法线采样点，就可以得到很不错的随视角改变的凹凸效果。另外可以通过光线步进的方式得到交点以解决遮蔽问题。

### 6. 阴影

非点光源（多光源、面光源、体积光源）会产生复杂的阴影效果，一般由本影和半影两部分构成。软阴影不能简单地通过对本影边缘做模糊化处理得到，正确的阴影效果是越接近物体的部分越实，远离的部分变虚。本影部分也不等同于点光源产生的硬阴影，当光源变大时，本影部分就变少，甚至完全消失。

通过投影矩阵变换可以得到被投射阴影物体在平面上的三角形面，绘制该三角形面即可得到阴影；不过有几点需要注意，首先是避免接受阴影的平面之下也被画上阴影，这个问题可以通过适度增加阴影三角形的偏移解决；另一种方法是先绘制平面，然后关闭深度缓冲直接绘制阴影三角形，最后再绘制其他部分，但如果阴影溢出平面，就可能会产生问题；解决方法是使用模板缓冲，只在接受阴影的表面绘制阴影三角形。

另一种绘制阴影的方法是将阴影三角形绘制到一张纹理图中，之后再将该纹理图应用到接受阴影的平面上，这种纹理图其实正是一类光照贴图。

如果光源位于阴影投射物体和阴影接受物体之间，或者阴影接受平面位于阴影投射物体之上，可能会产生反阴影，通过GPU管线的投影裁剪可以修正这个问题。

比较快的软阴影生成方法是使用卷积模糊硬阴影，但一般效果会很失真。Hanies的软阴影方法是对硬阴影的边缘从内向外由黑转白绘制近似的软阴影效果。

最常用的阴影投射方法是使用shadowmap，具有计算量可控、便于优化等优势，缺点是阴影质量和贴图分辨率挂钩，也和深度缓冲的数值精度挂钩；另外会存在例如shdow acne的问题，这个问题一般由两部分原因构成，一方面是精度问题，另一方面则是由于表面点采样来替代局部面上的深度值而造成的不精确问题（比如屏幕像素采样是从像素中心出发的），使用偏移系数可以一定程度解决这个问题。偏移有很多种方案，有沿着固定方向的、沿着背向光线的、沿着面法线的等，但偏移过多都会导致Peter Panning问题，表现为物体似乎像是浮起来。

为了解决对不同物体的阴影细节需求，级联阴影技术被提出，但级联阴影的使用需要对深度进行划分。一种划分方式是根据上一帧画面中的深度信息进行调整，但这无法处理新出现的物体；另一种处理方法是统计不同深度下的物体分布信息，但可能会产生阶跃的情况。一般用得最多的是第一种方案。级联阴影还会出现一些突变问题，可以通过混合或者抖动法解决。

PCF的思路恰好和光照过程相反，它通过表面某点观察光源的可见性来计算阴影的软硬。对于表面某点，PCF会在它周围相同深度生成多个采样点，根据这些点对光源的可见性来决定阴影的软硬程度。PCF的一个问题是，由于周围采样点的半径范围不变，所以软阴影区域固定不变，存在失真问题。

PCSS的主要思路在于，通过投射阴影物体的距离来决定PCF的采样半径。当投射阴影的物体距离阴影接受面变远时，采样半径就会增大，软阴影区域也会更大。（接触处阴影更硬，远离处阴影更软）

使用VSM可以对shadowmap进行滤波，它使用两张贴图分别存储深度值和深度值的平方。VSM可以显著提升运算效率，因为它可以利用硬件进行滤波。但VSM在一些特殊情况下会存在漏光现象。

例如烟雾、头发等模型需要产生自阴影，一种解决方案是使用带深度分片的阴影贴图，但这类贴图的开销较大，且存储和更新需要依赖于压缩算法及硬件设施。

不规则深度缓冲技术（IZB）将多个阴影接受物体存储到阴影贴图中，即每个纹素中可能存储了零个或多个阴影接受坐标。首先从视角空间产生深度信息，然后将该深度信息转换到光源空间下，此时就得到了光源的不规则深度缓冲，每个纹素中存储了所有可以被看到的阴影接受坐标信息；最后使用保守光栅化的策略绘制阴影。使用不规则深度缓冲技术可以生成非常完美的硬阴影，不会有Peter Panning等问题；为了叠加上软阴影效果，实际运用中也经常使用混合了IZB和PCSS的技术。

### 7. 光照和颜色

#### 7.1 辐射度量

辐射度量用于测定电磁辐射。不同波长的电磁波通常具有不同的性质。人眼可见的波长范围约为400nm~700nm。

辐射度量的基本单位为“**辐射通量**”（radiant flux），常用字母$\Phi$表示，描述单位时间内辐射的能量，单位为“瓦特”（$W$）。

“**辐照度**”（irradiance），常用字母$E$表示，描述单位面积上的辐射通量，表达式为$d\Phi/dA$，单位为“瓦特每平方米”（$W/m^2$）。

“**辐射强度**”（radiant intensity），常用字母$I$表示，描述单位立体角内的辐射通量，表达式为$d\Phi/d\omega$，单位为“瓦特每立体角”（$W/sr$）。

“**辐射度**”（radiance），常用字母$L$表示，描述单位立体角内单位面积上的辐射通量，表达式为$d^2\Phi/dAd\omega$，单位为“瓦特每平方米每立体角”（$W/m^2sr$）。（注意：这里的面积是按照垂直于光线的面积计算的，实际计算时需要根据光线方向使用余弦值修正）

辐射度是传感器（例如人眼、相机传感器等）对电磁辐射的度量方式。环境中的辐射度可以用具有5个变量的函数（考虑波长就是6个）描述，即3个描述空间坐标、2个描述方向。辐射度在公式中长表述为$L_o(\textbf{x},\textbf{d})或者L_i(\textbf{x},\textbf{d})$，即离开或进入$\textbf{x}$的辐射度，按照惯例，其中的$\textbf{d}$都采用离开$\textbf{x}$的方向。另外值得一提的是，辐射度的传播不随距离发生衰减。

光线传播中通常包含很多不同波长的电磁波，这可以通过光谱功率分布图看出。

每一种辐射度量都有其对应的光谱分布，由于光谱分布是在波长轴上的，所以光谱分布版的辐射量的单位就是加上一个“每纳米”（例如“瓦特每平方米每纳米”）。

#### 7.2 光度测定

辐射度量是基于物理的电磁辐射测量方式，并未考虑人眼的感知因素。光度测定和辐射度量很相似，但它根据人眼对不同波长电磁波的敏感程度，对测量结果进行权重修正。将辐射度量的计算结果乘以**CIE测光曲线**，即可转换为光度测定结果。

CIE测光曲线是一条以555nm波长为中心的钟形曲线，描述了人眼对不同波长电磁波的敏感程度。

同样的，每一种辐射度量都有其对应的光度测定量：

辐射通量→**光通量**（luminous flux），单位为流明（lumen，lm）

辐照度→**光照度**（illuminance），单位为勒克斯（lux，lx）

辐射强度→**光照强度/照度**（luminous intensity），单位为坎德拉（candela，cd）

辐射度→**光度/亮度**（luminance），单位为尼特（nit）

逻辑上来说应当以流明为光度测定的基本单位，但因为一些历史原因，坎德拉被用作基本单位，其他单位都从坎德拉导出。

光度常被用于描述平面的亮度，比如HDR电视屏幕的峰值亮度一般在500~1000nits，天空的亮度一般是8000nits，一个60瓦的灯泡的亮度一般是120000nits，而处于地平线上的太阳的亮度是600000nits。

#### 7.3 色度学

色度学主要用于建立光谱功率分布和颜色感知之间的联系。

人类可以区别大约一千万种不同的颜色。人眼通过视网膜内的三种不同视锥细胞感知不同波长的电磁波。其他动物的视锥细胞种类数可能和人类不同，有些多的可以达到十五种。所以，对于人眼来说，只需要三个数值就可以精确地描述所有可以感知的颜色。

CIE（国际照明委员会）设计了一套实验用于寻找这三种颜色，一组测试结果是645nm红波、526nm绿波和444nm蓝波，通过这三种颜色拟合各波长可见光的函数被称为“颜色匹配函数”。它将光谱功率分布转换成三个数值的组合。

但这三种颜色不能直接用于表达所有的可见光颜色，对于一些波长的颜色，可能会需要负数值描述。CIE于是提出了一种假想的三原色，使得所有波长的颜色都可以用正数值描述，这三种假想三原色一般表示为$\bar{x}(\lambda),\bar{y}(\lambda),\bar{z}(\lambda)$，其中的$\bar{y}(\lambda)$和CIE测光曲线相同。通过对光谱功率分布按假想的三原色积分，就可以得到CIE XYZ颜色空间。为了将颜色分解成亮度和色度，需要进一步地将颜色空间描述成一个二维的色度空间，当固定亮度为1之后，可以得到$X+Y+Z=1$这个色度平面。

色度坐标$x=\frac{X}{X+Y+Z}$

色度坐标$y=\frac{Y}{X+Y+Z}$

得到的(x,y)会形成一条曲线，这条曲线是所有的可见光谱，连接两个端点的直线被称为“紫线”，中间的点代表自然白光，这张图被称为“CIE色品图”。任取一点颜色，从白色点过该点向边缘连一条直线，到边缘长度的占比即表示该颜色的纯度，越接近边缘纯度越高；边缘上的点表示主波长。图形学中一般不用纯度和主波长描述，而是用饱和度和色相描述，虽然它们只是有点关系。

除了色度之外，剩下的一个用来描述颜色的维度就是亮度，用$Y$表示，这就被称为$xyY$坐标系。

电视或显示器使用特定光谱功率分布的RGB值作为显像基色，按不同值混合后形成目标颜色。

色品图中的三角形用来框定一个颜色域，用作显示设备的色域标准。色域三角形的三个顶点是三原色，是显示设备所能显示的饱和度最高的R、G、B颜色。用线段连接这三个顶点，其内部的所用颜色都可以通过三原色混合得到，这也是它们所形成的色域范围。实际上，显示设备的完整色域是三维的，色品图中展示的三角形只是它在二维平面上的投影。

不同RGB色彩空间通常是在CIE 1976 UCS色品图中对比的；因为XYZ空间中不同颜色对之间的差距可能会非常大，与人的感知不符，CIE对此进行了修正，给出了CIELUV色彩空间，颜色之间的距离尺度更符合人眼的感知，而CIE 1976 UCS正是CIELUV的一部分。后续有关颜色距离感知尺度的研究提出了例如$IC_TC_P和J_Za_Zb_Z$颜色空间，它们的距离感知尺度比CIELUV更精确，特别是对于高亮度和高饱和度的颜色，但目前尚未普遍使用。

在不同的颜色空间下进行渲染，得到的结果也不尽相同；例如在更广色域DCI-P3或者ACEScg中执行颜色计算可以得到比在线性sRGB下执行相同计算更精确的结果。

实际上，人眼对色彩的感知还依赖于环境光照、周边颜色以及其他一些先决条件，“色貌模型”（Color appearance models）就是一类用于预测颜色感知的模型。

#### 7.4 使用RGB进行渲染

严格来说，RGB是一种感知模型，用它进行基于物理的渲染是不正确的；正确的方式是使用光谱渲染，最后再通过密集采样或者坐标映射的方式转换到RGB颜色空间。

举例来说，物体表面对光线的反射，实际上会因为波长的不同而具有不同的反射率，这种特性一般用“光谱反射曲线”描述。正确的反射计算应当对光谱功率分布中的每个波长乘以其对应的光谱反射曲线得到，然后再转换到RGB色彩空间。而完全使用RGB色彩空间的光照计算则直接对RGB颜色进行混合来得到反射结果，这并不正确，但现实中大部分材质的光谱反射曲线都比较平滑，这也使得完全使用RGB的颜色计算结果误差较小。

但对于一些执行预测渲染的应用来说，这种误差就很重要了。两条不同的光谱反射曲线可能在某一种光源下表现为相同的颜色，但在另一种光源下就完全不同，这种现象被称为“异光源色度差”，对于汽车补漆等应用场景来说是非常重要的。

#### 7.5 HDR

HDR显示器使用Rec. 2020和Rec. 2100色域标准。Rec. 2020使用比Rec.709和sRGB更宽的色域范围，但相同的白值；Rec. 2100则定义了两个非线性的显示编码：感知量化器（PQ）、混合对数伽马（HLG），渲染领域一般使用PQ，它定义了最大峰值亮度为$10,000cd/m^2$。

实际上，当前主流的消费级HDR显示器很难达到需要用Rec. 2020评估的程度，人们往往更倾向于使用DCI-P3标准衡量HDR显示器的色域范围。使用HDR显示图像通常有三种方案（并不是所有的应用程序和操作系统都完全支持所有方案）：

- HDR10：使用32bit无符号整型存储每个像素的颜色，即R10G10B10A2，使用PQ编码和Rec. 2020色域空间，HDR10的映射没有统一标准。
- scRGB（线性）：仅支持Windows操作系统，通常按照sRGB标准显示，但允许数值低于0或者超过1，使用每个颜色通道16bit的线性值，可以兼容HDR10，该方案主要是为了向后兼容sRGB。
- 杜比视界（Dolby Vision）：专用格式，并不是一种广泛支持的格式。使用每个颜色通道12bit存储颜色值，使用PQ编码和Rec. 2020色域空间。内部映射是标准化的，但没有文档。

将像素RGB值转换到Rec. 2020色域空间并使用PQ编码，通常产生比sRGB高很多的开销，有一些优化方法可以快速近似求解。另外，使用HDR显示的时候，需要注意调节UI的显示效果，避免过亮或者过暗。

#### 7.6 色调映射

色调映射将场景辐射度转换为显示辐射度。这一步转换被称为“端到端转换”或者“场景到屏幕转换”。使用“图像状态”来描述映射过程，两个基本的图像状态分别为“场景图像”和“显示图像”，“场景图像”是用场景辐射度描述的图像，”显示图像“是用显示辐射度描述的图像；图像状态与显示编码无关，无论哪一种图像状态都可以进行线性或非线性的显示编码。

色调映射并不是一种从场景到屏幕的恒等变换，也不是为了将场景的颜色空间压缩到显示器的颜色空间（尽管两者的动态颜色范围确实需要考虑）。色调映射是根据观看环境和显示特性，提供一种尽可能接近于临场体验的视觉颜色效果。

但还原临场感实际上是非常困难的，现实中的环境亮度可能比显示设备所支持的最大亮度高出几个数量级，颜色饱和度也可能远远超出显示设备所支持的范围。为了实现这样的效果，就需要充分利用人眼的视觉系统。

人眼视觉系统的适应能力可以补偿绝对亮度上的差距，但这种补偿是有缺陷的，在亮度较低的情况下会使对比度下降，也就是感受到的”视彩度“减少（亨特效应）。

环境因素也会影响感知对比度，例如显示设备所处房间的照明条件、屏幕反光等。

以上现象说明，为了达到临场感的效果，就需要提高显示图像的对比度和饱和度。

提高对比度会进一步地压缩亮度范围，这就需要通过渐入渐出来补充暗部和亮部地细节，也就是使用S型映射曲线，这和光化学胶片的灵敏度曲线是一致的，也就是讨论色调映射时常说的”胶片质感“。

曝光对于色调映射来说也是至关重要的，但是如何确定曝光系数是设计色调映射时的一个棘手问题。一般来说，色调映射应当以一种确定的曝光方式映射整个场景，这种色调映射是一种”全局色调映射“；另一种”局部色调映射“则对不同区域的像素使用不同的曝光系数。实时渲染中一般都使用全局色调映射。

对颜色的非线性映射可能会产生饱和度与色调的偏移，尽管这种偏移有助于抵消亨特效应，但现代的色调映射还是倾向于消除这种偏移并在色调映射之后使用额外的颜色校正。

对亮度的映射几乎不产生饱和度与色调的偏移，但亮度可能会超出显示器的色域，需要映射回色域范围。

### 8. 基于物理的着色

#### 8.1 物理光照

在物理光学中，光被描述为一种横波（电磁波），是一种垂直于传播方向、使电场和磁场振动的波。电场和磁场的振动是相互激发的，它们的矢量相互垂直且长度的比值固定为相位速度。

现实中大多数的光都是非偏振光，也就是电场和磁场的振动会沿着垂直于传播轴的各个方向均匀分布。光在真空中的相位速度为*c*，约为$300,000 km/s$，常被称为“光速”。光波带有能量，能量流的密度（能流密度）等于电场大小和磁场大小的乘积。在渲染领域中更关注“平均能流密度”（辐照度，单位面积上的辐射通量），它与波幅的平方成正比，用$E$表示。

光波是线性叠加的，这可能看起来和辐照度的描述相矛盾，有时也确实会出现诸如“1+1=4”这样的情况。举例来说，将$n$个仅相位不同的单色光波叠加，当它们刚好相差若干个波长时，它们叠加得到的光波波幅也达到了原来的$n$倍（相长干涉），辐照度也就变成了原来的$n^2$倍；当它们互相抵消时（比如依次相差半个波长），叠加的波幅为零（相消干涉），辐照度也为零；某种特殊的情况下，它们叠加的波幅刚好是原来的$\sqrt{n}$倍，则辐照度也就变成了原来的$n$倍。同样的，这也并不违背能量守恒定律，光波在空间中的叠加会使某些区域上的能量大于各光波能量的线性和，在另外一些区域上则会小于各光波能量的线性和，它们相互抵消，最终表现为各光波的能量总和不变。

> 电磁波或光波是粒子相互碰撞时，电子绕原子核振动时产生的波动。
>
> 电子振动的周长决定了电子发出的电磁波或光波的波长；电子振动的时间周期决定了电子发出的电磁波或光波的周期频率；电子振动速度几乎等同于光速；电磁波或光波是电子的周期性运动带动时空运动产生的波动或粒子，并不是电子本身。

当物体中的电子振动时就会产生光波，引起振荡的能量（热能、电能、化学能等）就转换为了光能，从物体中辐射出去。在渲染中，这类向外辐射光能的物体就是光源。光波一旦被发射出去，就会在空间中不断传播，直到遇到其他物质。光波和绝大多数物质的作用都是简单的，光波的电场会带动物质中的电子振动，振动的电子又会产生新的光波向外辐射，这就表现为入射光波的一部分能量被调转了方向，也就是所谓的“散射”，是最常见的一种光学现象。

散射光波的频率是和原始光波相同的。一般情况下，原始光波包含了各种频率的光波，每一种频率的光波都单独地和物质发生作用。一种频率的入射光波能量对另一种频率的出射光波能量没用贡献，除了极少数的光致发光现象（荧光、磷光）。

一个孤立的分子会向各个方向散射光波，但在不同方向上有强度的变化。大部分光波都沿着靠近原始传播的方向散射，包括正向和反向的散射。分子附近的光波被散射的可能性很大程度上受波长的影响，短波比长波更可能被散射。

实际渲染中更关注于大量分子的集合体，其所体现的与光波的相互作用可能与孤立的分子不同。来自于附近分子的散射光波通常是相干的，也就表现为光的干涉（因为有相同的入射光）。

#### 8.2 粒子模型

在理想气体中，分子做无规则运动且不会产生相互作用。这种模型下，不同分子散射光波的相位差是随机的且不断变化的，也就表现为不同散射光波之间的不相干，能量上呈线性叠加。换句话说，这种模型下由$n$个分子散射的光波能量就是单个分子的$n$倍。

另一种情况下，如果分子被紧密地排列成小于光波波长的一个个小簇，那么每个分子簇散射的光波就是相干的，则散射的光波能量会随光波数量的增长呈平方级增加。也就是说，一小簇聚在一起的分子会散射出$n$倍于理想气体中同等数量分子散射的光波能量。

以上两种情况说明，对于立方米体积中固定的分子密度来说，将分子聚成簇可以显著增加散射光的强度。在整体分子密度不变的情形下继续增大分子簇，可以进一步增强散射光的强度，直到分子簇的直径接近光的波长。当分子簇的直径超过波长后，再增加分子簇直径并不会增强散射光的强度。这种现象解释了云层和雾的散射效果，它们实质上是由空气中的水分子聚集形成的分子簇，即使空气中的水分子密度不变，云和雾也可以形成强烈的光线散射效果。

这类基于粒子模型的散射被称为大气的“瑞利散射”（Rayleigh scattering）或者固体颗粒的“丁达尔散射”（Tyndall scattering）。

当大气中的分子簇（粒子）直径继续增大到超过波长时，散射特性会发生改变，散射的光线向前方比向后方更强，且对波长的依赖性降低，直到所有可见光都可以均匀地散射。这类散射被称为“米氏散射”（Mie scattering）。

#### 8.3 介质模型

另一种重要的情形是穿过均匀介质（具有均匀间隔的同种分子形成的区域）的光线传播。介质中的分子不要求像在晶体中那样完全规则，不含有气泡和间隙的液体或非晶体纯净物，都可以是光学均匀的介质。

在均匀介质中，散射光波排列整齐，在除了初始传播方向的其他所有方向上都是非相干的。原始光波与所有散射的光波结合后，基本和原始光波相同，除了相位速度以及某些情形下的振幅可能略有区别。最终体现为未发生任何的散射现象，因为散射现象被非相干性抑制了。

原始光波的相位速度和新光波的相位速度的比值定义了该介质的一种光学特性——折射率（IOR），用$n$表示。一些介质具有吸收特性，它们会将部分光波能量转换为热能，从而使得介质中的光波振幅随行进距离呈指数级下降，下降的速率用衰减系数（attenuation index）定义，用$\kappa$表示。不同波长光的$n和\kappa$都不相同，这两个数值定义了不同介质对特定波长光波的作用效果，它们通常结合在一起通过复数$n+i\kappa$表示，被称为复折射率。

介质对光线的吸收会直接影响视觉效果，表现为光线强度的减弱甚至是颜色的变化。

非均匀介质可以当作带有散射粒子的均匀介质。光波的非相干性是由均匀介质中分子的均匀分布引起的，任何局部的分布变化都可以打破这种非相干性。这种局部的分布变化可以是不同类型的分子、气泡或是密度变化。

介质对光线的散射和吸收是与规模有关的。一种介质在小规模下不产生可见的散射，可能会在大规模下产生非常显著的散射效果。比如在空气和水中的光线散射在房间内或杯子中是很难观察到的，但在自然环境中可能会非常显著。

#### 8.4 表面模型

从物理光学角度来看，物体的表面实际上是分离两个具有不同折射率体积的二维平面。最典型的情况下，外层的体积是包含空气的，也就是折射率约为1.003；内层的体积则需要根据物质构成来确定折射率。

当光波抵达物体表面时，通常由两个因素决定它们之间的作用：表面两侧的物质、表面的几何特征。

首先假设表面是一个理想的完美平面，只考虑表面两侧物质的作用，并将外侧的折射率用$n_1$表示，将内侧的折射率用$n_2$表示。平面对光线的散射是在一种特定的情况下发生的，边界条件是光波的电场与平面平行，也就是光波的电场在物体表面的投影需要保证内外一致，这其中包含了几个隐藏条件：

- 散射的光波和原始光波是同相的，或者180°反相；也就是同相时的透射，或者反相时的反射。
- 散射的光波和原始光波是同频的，非偏振光可以看成多个偏振光的组合。
- 当光波从一种介质进入另一种介质时，它的相位速度会随相对折射率（$n_1/n_2$）成比例地变化；由于频率不变，所以波长也随之变化。

折射光线与法线的夹角$\theta_t$符合斯涅尔定律：$sin(\theta_t)=\frac{n_1}{n_2}sin(\theta_i)$

即使不透明的物体也会发生折射。光线折射进金属内部后，会被金属内部的大量自由电子吸收，重新向反射的方向辐射出去，所以金属同时具备高吸收率和高反射率。

除了发生突变的折射以外，也有折射率发生缓慢变动的情形，例如温度变化引起的空气密度变化就会产生热流扰动。

即使是具有良好定义边界的物体，如果被浸没在具有完全相同折射率的物质中也会变得表面不可见。没有折射率的变化就没有反射和折射现象。

除了表面两侧物质的作用，另一个决定因素是表面的几何特征。比波长小的不规则性并不会影响光照效果；而比波长大得多的不规则性只会将局部表面倾斜，同样也不会影响光照效果；只用不规则平面的尺度在1~100倍波长的情况下，会使得表面看上去和理想平面很不一样，因为会产生衍射效应。

渲染领域中一般使用几何光学，也就是不考虑干涉、衍射之类的波动效应。这相当于假定了物体表面的不规则性都是小于波长或者远大于波长的。

即使是远大于波长的尺度，很多时候对于一个屏幕像素来说还是太小了，但光线又是从各个方向汇聚而来的，所以渲染中一般用统计学方式来描述微小几何结构的不规则性对宏观表面的影响，也就是微尺度的粗糙度。

**次表面散射**：进入金属表面的大部分物体会被反射出去，少部分折射进入内部的光线也会被快速吸收。但对于非金属物体来说，光线的散射和吸收就有很大的差别。散射程度和吸收率比较低的物质就会呈现为透明或者半透明，折射进入内部的光线会穿透整个物体；光线进入不透明的物体后会在发生多次散射和吸收后，重新从表面出去，这种现象被称为次表面散射。

光线离开表面时的位置可能距离入射点很近也可能较远，这主要取决于物质的散射特性。当着色尺度（像素尺寸或者采样点间距）大于这种距离时，可以当作光线从入射点射出，这样可以很容易地在光照着色模型中使用额外的反射项来描述。高光项（specular term）是用来描述表面反射的，漫反射项（diffuse term）是用来描述局部次表面散射的。但如果着色尺度小于入射点和出射点之间的距离，就需要专门处理这种视觉效果，被称为“全局次表面散射”技术。

